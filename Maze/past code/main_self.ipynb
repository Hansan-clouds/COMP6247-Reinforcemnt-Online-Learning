{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZdoFMeq6_vlW","outputId":"fbbcca23-3186-4df4-fa2e-94895096bb41","executionInfo":{"status":"ok","timestamp":1653515710200,"user_tz":-60,"elapsed":16125,"user":{"displayName":"HAN ZHANG","userId":"00984646561456990334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X48QpV7G_0oi","outputId":"996e778f-2c8f-4f48-8715-401a3f835000","executionInfo":{"status":"ok","timestamp":1653515710201,"user_tz":-60,"elapsed":9,"user":{"displayName":"HAN ZHANG","userId":"00984646561456990334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Maze/Maze\n"]}],"source":["cd /content/drive/MyDrive/Maze/Maze"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mVCzK7TpADlz","outputId":"66eb3fcf-525a-4132-8341-ebb7011b67d1","executionInfo":{"status":"ok","timestamp":1653515715807,"user_tz":-60,"elapsed":5610,"user":{"displayName":"HAN ZHANG","userId":"00984646561456990334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pygame\n","  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n","\u001b[K     |████████████████████████████████| 21.8 MB 6.8 MB/s \n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.1.2\n"]}],"source":["!pip install pygame"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtS6Jz75j-B1","outputId":"ec02bf5d-ff5f-40e6-a0ff-a5affa960bb7","executionInfo":{"status":"ok","timestamp":1653515718443,"user_tz":-60,"elapsed":2645,"user":{"displayName":"HAN ZHANG","userId":"00984646561456990334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["pygame 2.1.2 (SDL 2.0.16, Python 3.7.13)\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n"]}],"source":["import os\n","import torch as T\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from read_maze import load_maze, get_local_maze_information\n","import pygame\n","import sys\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMmRGFhpZCCv"},"outputs":[],"source":["import os\n","os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5IMi-1W_oj7"},"outputs":[],"source":["\n","\n","SCREENSIZE = W, H = 1200, 800\n","mazeWH = 800\n","origin = ((W - mazeWH)/2, (H - mazeWH)/2)\n","lw = 2 # linewidth of maze-grid\n","\n","# Colors\n","GREY = (140,140,140) # (15,15,15)\n","DARKGREY = (27, 27,0)\n","RED = (255, 0, 0)\n","BLUE = (0, 0, 255)\n","GREEN = (0, 255, 0)\n","DARKGREEN = (0, 150, 0)\n","BLACK = (0, 0, 0)\n","WHITE = (255, 255, 201)\n","\n","\n","class Canvas:\n","    def __init__(self):\n","        self.step_cntr = 0\n","        self.cntr = 0\n","\n","        self.maze = load_maze()\n","        self.shape = 201\n","\n","        pygame.init()\n","        self.surface = pygame.display.set_mode(SCREENSIZE)\n","        self.actor = (1,1)\n","\n","    def drawSquareCell(self, x, y, dimX, dimY, col=(0, 0, 0)):\n","        pygame.draw.rect(\n","            self.surface, col,\n","            (x, y, dimX, dimY)\n","        )\n","\n","    def drawSquareGrid(self, origin, gridWH):\n","        CONTAINER_WIDTH_HEIGHT = gridWH\n","        cont_x, cont_y = origin\n","\n","        # DRAW Grid Border:\n","        # TOP lEFT TO RIGHT\n","        pygame.draw.line(\n","            self.surface, BLACK,\n","            (cont_x, cont_y),\n","            (CONTAINER_WIDTH_HEIGHT + cont_x, cont_y), lw)\n","        # # BOTTOM lEFT TO RIGHT\n","        pygame.draw.line(\n","            self.surface, BLACK,\n","            (cont_x, CONTAINER_WIDTH_HEIGHT + cont_y),\n","            (CONTAINER_WIDTH_HEIGHT + cont_x,\n","             CONTAINER_WIDTH_HEIGHT + cont_y), lw)\n","        # # LEFT TOP TO BOTTOM\n","        pygame.draw.line(\n","            self.surface, BLACK,\n","            (cont_x, cont_y),\n","            (cont_x, cont_y + CONTAINER_WIDTH_HEIGHT), lw)\n","        # # RIGHT TOP TO BOTTOM\n","        pygame.draw.line(\n","            self.surface, BLACK,\n","            (CONTAINER_WIDTH_HEIGHT + cont_x, cont_y),\n","            (CONTAINER_WIDTH_HEIGHT + cont_x,\n","             CONTAINER_WIDTH_HEIGHT + cont_y), lw)\n","\n","\n","    def placeCells(self):\n","        # GET CELL DIMENSIONS...\n","        cellBorder = 0\n","        celldimX = celldimY = (mazeWH / self.shape)\n","\n","        # DOUBLE LOOP\n","        for rows in range(201):\n","            for cols in range(201):\n","                # Is the grid cell tiled ?\n","                if (self.maze[rows][cols] == 0):\n","                    self.drawSquareCell(\n","                        origin[0] + (celldimY * rows)\n","                        + cellBorder + lw / 2,\n","                        origin[1] + (celldimX * cols)\n","                        + cellBorder + lw / 2,\n","                        celldimX, celldimY, col=BLACK)\n","                if cols == 199 and rows == 199:\n","                    self.drawSquareCell(\n","                        origin[0] + (celldimY * rows)\n","                        + cellBorder + lw / 2,\n","                        origin[1] + (celldimX * cols)\n","                        + cellBorder + lw / 2,\n","                        celldimX, celldimY, col=BLUE)\n","\n","    def step(self, visible, idx, path):\n","        \"\"\"Run the pygame environment for displaying the maze structure and visible (local) environment of actor\n","        \"\"\"\n","        self.get_event()\n","        self.set_visible(visible, idx, path)\n","\n","        self.surface.fill(GREY)\n","        self.drawSquareGrid(origin, mazeWH)\n","\n","        self.placeCells()\n","        self.draw_visible()\n","        pygame.display.update()\n","        self.step_cntr += 1\n","\n","    def set_visible(self, visible, idx, path):\n","        self.vis = visible\n","        self.actor = idx\n","        self.path = path\n","\n","    def draw_visible(self):\n","        \"\"\"Draw the visible environment around the actor\n","\n","        Notes\n","        -----\n","        RED - signifies a fire\n","        DARKGREY - signifies a visible wall\n","        WHITE - signifies a path\n","        GREEN - indicates the actor's position\n","        \"\"\"\n","        celldimX = celldimY = (mazeWH / self.shape)\n","\n","        #self.visible = self.vis\n","        for s in self.path:\n","            self.drawSquareCell(\n","                origin[0] + (celldimY * s[1])\n","                + lw / 2,\n","                origin[1] + (celldimX * s[0])\n","                + lw / 2,\n","                celldimX, celldimY, col=DARKGREEN)\n","\n","\n","        for row in range(3):\n","            for col in range(3):\n","                c = self.actor[0] + (col - 1)\n","                r = self.actor[1] + (row - 1)\n","\n","\n","                if row == 1 and col == 1:\n","                    self.drawSquareCell(\n","                        origin[0] + (celldimY * r)\n","                        + lw / 2,\n","                        origin[1] + (celldimX * c)\n","                        + lw / 2,\n","                        celldimX, celldimY, col=GREEN)\n","                else:\n","                    if self.vis[row][col][1] > 0:\n","\n","                        self.drawSquareCell(\n","                            origin[0] + (celldimY * r)\n","                            + lw / 2,\n","                            origin[1] + (celldimX * c)\n","                            + lw / 2,\n","                            celldimX, celldimY, col=RED)\n","                    # else:\n","                    #     if self.vis[row][col][0] == 1:\n","                    #         self.drawSquareCell(\n","                    #             origin[0] + (celldimY * r)\n","                    #             + lw / 2,\n","                    #             origin[1] + (celldimX * c)\n","                    #             + lw / 2,\n","                    #             celldimX, celldimY, col=WHITE)\n","                    #     elif self.vis[row][col][0] == 0:\n","                    #         self.drawSquareCell(\n","                    #             origin[0] + (celldimY * r)\n","                    #             + lw / 2,\n","                    #             origin[1] + (celldimX * c)\n","                    #             + lw / 2,\n","                    #             celldimX, celldimY, col=DARKGREY)\n","\n","    def get_event(self):\n","        for event in pygame.event.get():\n","            if event.type == pygame.QUIT:\n","                sys.exit()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-4XTB9jPVMw"},"outputs":[],"source":["'''\n","代码的来源\n","https://github.com/alvaroprat97/RL-Maze-Solver/blob/master/Complex_Maze/agent.py\n","https://github.com/luispsantos/EL2805-Reinforcement-Learning/blob/main/lab2/lab2_instructions.pdf\n","https://zhuanlan.zhihu.com/p/346165057\n","https://github.com/xiaoyw71/Reinforcement-learning-practice\n","https://github.com/luispsantos/EL2805-Reinforcement-Learning/blob/4117514ae3cb3003df3785db64a9dc1d3a2b6954/lab1/problem1/maze.py\n","Source - https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/CombinedExperienceReplay/memory_solution.py\n","https://github.com/lukun199/DQN_maze/blob/main/Main.py\n","'''\n","import matplotlib.pyplot as plt\n","import time\n","from IPython import display\n","global action_dir\n","action_dir = {\"0\": {\"id\":'stay',\n","                    \"move\":(0,0)},\n","              \"1\": {\"id\":'up',\n","                    \"move\":(0,-1)},\n","              \"2\": {\"id\":'left',\n","                    \"move\":(-1,0)},\n","              \"3\": {\"id\":\"down\",\n","                    \"move\":(0,1)},\n","              \"4\": {\"id\":'right',\n","                    \"move\":(1,0)}\n","              }\n","\n","global rewards_dir\n","rewards_dir = {\"onwards\": -.04,\n","              \"backwards\":-0.0,\n","              \"visited\":-0.3,\n","              \"blockedin\":-0.1,\n","              \"fire\":-1.,\n","              \"wall\":-.75,\n","              \"stay\":-0.3,\n","              }\n","\n","LIGHT_RED = '#FFC4CC'\n","LIGHT_GREEN = '#95FD99'\n","BLACK = '#FFFFFF'\n","WHITE = '#000000'\n","LIGHT_PURPLE = '#E8D0FF'\n","LIGHT_ORANGE = '#FAE0C3'\n","\n","\n","class Qmaze:\n","    def __init__(self):\n","        # Reset the total number of steps which the agent has taken\n","        self.num_steps_taken = 0\n","        \n","        self.rat = (1,1)\n","        self.goal = (199,199)\n","        self.rat_path = [self.rat]\n","        self.around_info = []\n","        self.observation = self.observe_environment\n","\n","        \n","    @property\n","    def reset(self):\n","        self.num_steps_taken = 0\n","        \n","        self.rat = (1,1)\n","        self.goal = (199,199)\n","        self.rat_path = [self.rat]\n","        self.around_info = []\n","        self.observation = self.observe_environment\n","\n","\n","    @property\n","    def get_loc_info(self):\n","        return self.observation\n","\n","    @property\n","    def get_rat_pos(self):\n","        return self.rat\n","\n","    @property\n","    def get_rat_path(self):\n","        return self.rat_path\n","\n","    @property\n","    def get_around_info(self):\n","      col, rol = self.get_rat_pos\n","      location = get_local_maze_information(rol,col)\n","      self.around_info = location.copy()\n","      return self.around_info\n","\n","    # def get_around_info(self):\n","    #   return self.around_info\n","    @property\n","    def observe_environment(self):\n","        col, rol = self.get_rat_pos\n","        location = get_local_maze_information(rol,col)\n","        # self.set_around_info\n","        temp = []\n","\n","        for i in range(location.shape[0]):\n","            for j in range(location.shape[1]):\n","                if location[i][j][0] == 0:\n","                    temp.append(0) #wall\n","                elif location[i][j][0] == 1 and location[i][j][1] ==0:\n","                    temp.append(1) #no fire and empty\n","                elif location[i][j][0] == 1 and location[i][j][1] >0:\n","                    temp.append(location[i][j][1]) #fire\n","                else:\n","                    temp.append(1) #自己的位置\n","\n","        self.rat_path.append(self.rat)\n","        self.observation = temp\n","        return self.observation\n","\n","    \n","    def animate_solution(self):\n","      maze = load_maze()\n","      path = self.get_rat_path\n","    # Map a color to each cell in the maze\n","      col_map = {0: WHITE, 1: BLACK, 2: LIGHT_GREEN, -6: LIGHT_RED, -1: LIGHT_RED}\n","\n","    # Size of the maze\n","      rows, cols = maze.shape\n","\n","    # Create figure of the size of the maze\n","      fig = plt.figure(1, figsize=(cols, rows))\n","\n","    # Remove the axis ticks and add title title\n","      ax = plt.gca()\n","      ax.set_title('Policy simulation')\n","      ax.set_xticks([])\n","      ax.set_yticks([])\n","\n","    # Give a color to each cell\n","      colored_maze = [[col_map[maze[j, i]]\n","                     for i in range(cols)] for j in range(rows)]\n","\n","    # Create figure of the size of the maze\n","      fig = plt.figure(1, figsize=(cols, rows))\n","\n","    # Create a table to color\n","      grid = plt.table(cellText=None,\n","                     cellColours=colored_maze,\n","                     cellLoc='center',\n","                     loc=(0, 0),\n","                     edges='closed')\n","\n","    # Modify the hight and width of the cells in the table\n","      tc = grid.properties()['children']\n","      for cell in tc:\n","          cell.set_height(1.0/rows)\n","          cell.set_width(1.0/cols)\n","\n","    # Update the color at each frame\n","      for i in range(len(path)):\n","\n","        ax.set_title(f'\\t \\t \\t \\t  Policy simulation \\t \\t \\t t {i} T {len(path)-1}'.expandtabs())\n","        # First clear the prev illustration, if path[i] is same as path[i-1] then it is already changed!\n","        # Illustration of current status\n","        if i > 0:\n","        # if path[i][0:2] != path[i-1][0:2]:\n","            grid.get_celld()[(path[i-1])].set_facecolor(col_map[maze[path[i-1]]])\n","\n","\n","        # Agent illustration\n","        grid.get_celld()[(path[i])].set_facecolor(LIGHT_ORANGE)\n","        grid.get_celld()[(path[i])].get_text().set_text('Rat')\n","\n","\n","\n","        # Position is the same and it is WIN!\n","        # if maze[path[i][0:2]] == 2:\n","        #     grid.get_celld()[(path[i][0:2])].set_facecolor(LIGHT_GREEN)\n","        #     grid.get_celld()[(path[i][0:2])].get_text().set_text('WIN')\n","        #     break # Since nothing changes\n","\n","        display.display(fig)\n","        display.clear_output(wait=True)\n","        time.sleep(1)\n","\n","    def step(self, action, score):\n","        \"\"\"Sample environment dependant on action which has occurred\n","\n","        Action Space\n","        ------------\n","        0 - no move\n","        1 - up\n","        2 - left\n","        3 - down\n","        4 - right\n","\n","        \"\"\"\n","        #time.sleep(1) # delay for time animation\n","        self.num_steps_taken += 1 # increment time\n","\n","        global action_dir # Fetch action directory containing the properties of each action w.r.t environment\n","        act_key = str(action)\n","        global rewards_dir # Fetch reward directory\n","\n","        x_inc, y_inc = action_dir[act_key]['move'] # fetch movement from position (1,1)\n","\n","        # If too much time elapsed you die in maze :( (terminate maze at this point)\n","        if score < -100:\n","            print('I became an old man and dies in this maze...')\n","            return self.observe_environment, -1., True # terminate\n","\n","        obsv_mat = self.get_around_info # get prior position\n","        x, y = self.rat\n","\n","        x_loc, y_loc = (1 + x_inc, 1 + y_inc) # Update Local Position\n","\n","        is_blocked = True\n","        for o in obsv_mat:\n","            for s in o:\n","                if s[0] == 1 and s[1] == 0: # if there is a path and no fire, we are not blocked\n","                    is_blocked = False\n","\n","        if action_dir[act_key]['id'] == 'stay' and is_blocked: # if we need to stay then reward\n","            return self.observe_environment, rewards_dir['blockedin'], False\n","        elif action_dir[act_key]['id'] == 'stay': # if we stay for no reason then penalise\n","            return self.observe_environment, rewards_dir['stay'], False\n","\n","        if obsv_mat[y_loc][x_loc][0] == 0: # check for a wall\n","            return self.observe_environment, rewards_dir['wall'], False\n","        if obsv_mat[y_loc][x_loc][1] > 0: # check for a wall\n","            return self.observe_environment, rewards_dir['fire'], True\n","\n","        # So if we do successfully move\n","        self.rat = new_pos = (x + x_inc, y + y_inc) # new global position if we move into a free space\n","        # Have we reached the end?\n","        if new_pos == (199, 199):\n","            return self.observation, rewards_dir['end'], True\n","\n","        # Have we visited this spot already?\n","        if self.rat in self.rat_path:\n","            return self.observe_environment, rewards_dir['visited'], False\n","\n","        # Are we moving towars the goal?\n","        if x_inc > 0 or y_inc > 0:\n","            return self.observe_environment, rewards_dir['onwards'], False\n","\n","        # finally our only choice is to move away from goal\n","        return self.observe_environment, rewards_dir['backwards'], False\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m715n9Yw_hrH"},"outputs":[],"source":["class ReplayBuffer():\n","    def __init__(self, input_shape, max_size, batch_size):\n","        self.batch_size = batch_size\n","        self.mem_size = max_size # bound memory so we don't crash RAM\n","        self.mem_cntr = 0 # simulate stack by knowing whee in memory we are\n","\n","        # initialise state memory with 0 values\n","        self.state_mem = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n","        self.state_mem_ = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n","\n","        self.action_mem = np.zeros(self.mem_size, dtype=np.int64)\n","        self.reward_mem = np.zeros(self.mem_size, dtype=np.float32)\n","        # self.terminal_mem = np.zeros(self.mem_size, dtype=bool)\n","\n","    def store_buffer(self, state, state_, reward, action):\n","        # wrap counter to know where we are in memory\n","        idx = self.mem_cntr % self.mem_size\n","        # assign states, reward and action to memory\n","        self.state_mem[idx] = state\n","        self.state_mem_[idx] = state_\n","        self.reward_mem[idx] = reward\n","        self.action_mem[idx] = action\n","        # increment position in memory\n","        self.mem_cntr += 1\n","\n","    def sample_buffer(self):\n","        # Max existing memory size (if mem not full set max mem to counter value)\n","        max_mem = min(self.mem_size, self.mem_cntr)\n","        btch = np.random.choice(max_mem, self.batch_size, replace=False)\n","\n","        states = self.state_mem[btch]\n","        states_ = self.state_mem_[btch]\n","        actions = self.action_mem[btch]\n","        rewards = self.reward_mem[btch]\n","\n","        return states, actions, rewards, states_\n","\n","    def is_sufficient(self):\n","        return self.mem_cntr > self.batch_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZIPyhFIeHfh"},"outputs":[],"source":["class Network(nn.Module):\n","    # The class initialisation function. This takes as arguments the dimension of the network's input (i.e. the dimension of the state), and the dimension of the network's output (i.e. the dimension of the action).\n","    def __init__(self, input_dims, n_actions, lr):\n","        # Call the initialisation function of the parent class.\n","        super(Network, self).__init__()\n","        # Define the network layers. This example network has two hidden layers, each with 100 units.\n","        self.layer_1 = T.nn.Linear(input_dims, 256)\n","        self.layer_2 = T.nn.Linear(256, 256)\n","        self.output_layer = T.nn.Linear(256,n_actions)\n","\n","        self.optimiser = optim.Adam(self.parameters(), lr=lr)\n","        self.loss = nn.MSELoss()\n","        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","\n","    # Function which sends some input data through the network and returns the network's output. In this example, a ReLU activation function is used for both hidden layers, but the output layer has no activation function (it is just a linear layer).\n","    def forward(self, input):\n","        layer_1_output = T.nn.functional.relu(self.layer_1(input))\n","        layer_2_output = T.nn.functional.relu(self.layer_2(layer_1_output))\n","        action = self.output_layer(layer_2_output)\n","        return action\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gaiEboHolfom"},"outputs":[],"source":["\n","class Agent():\n","    def __init__(self, gamma, epsilon, buffer_size, input_dims, n_actions, lr, mem_size, batch_size, eps_min=0.01, eps_dec=5e-7, replace=10):\n","        \n","        self.learn_step_counter = 0\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.lr = lr\n","        self.n_actions = n_actions\n","        self.input_dims = input_dims\n","        self.batch_size = batch_size\n","        self.eps_min = eps_min\n","        self.eps_dec = eps_dec\n","        self.replace_target_thresh = replace\n","        self.action_space = [i for i in range(self.n_actions)]\n","        self.buffer_size = buffer_size\n","        self.memory = ReplayBuffer(buffer_size, mem_size, batch_size)\n","        self.q_eval = Network(self.input_dims, self.n_actions, self.lr)\n","        self.q_next = Network(self.input_dims, self.n_actions, self.lr)\n","\n","    def greedy_epsilon(self, observation):\n","        # if we randomly choose max expected reward action\n","        if np.random.random() > self.epsilon:\n","            actions = self.q_eval.forward(T.tensor([observation], dtype=T.float).to(self.q_eval.device))\n","            action = T.argmax(actions).item()\n","        # otherwise random action\n","        else:\n","            action = np.random.choice(self.action_space)\n","        return action\n","\n","    \n","    def store_transition(self, state, state_, reward, action):\n","        self.memory.store_buffer(state, state_, reward, action)\n","\n","    def replace_target_network(self):\n","      dqn_dict =  T.nn.Module.state_dict(self.q_eval)\n","      T.nn.Module.load_state_dict(self.q_next,dqn_dict)\n","      \n","\n","    def dec_epsilon(self):\n","        self.epsilon = self.epsilon - self.eps_dec \\\n","            if self.epsilon > self.eps_min else self.eps_min\n","\n","\n","    def learn(self):\n","        # Wait for memory to fill up before learning from empty set\n","        if not self.memory.is_sufficient():\n","            return\n","\n","        # Start AD\n","        self.q_eval.optimiser.zero_grad()\n","        self.replace_target_network()\n","\n","        # sample memory\n","        state, actions, reward, state_ = self.memory.sample_buffer()\n","        idxs = np.arange(self.batch_size) # need for array slicing later\n","\n","        state_btch = T.tensor(state).to(self.q_eval.device)\n","        action_btch = T.tensor(actions).to(self.q_eval.device)\n","        rewards_btch = T.tensor(reward).to(self.q_eval.device)\n","        state_btch_ = T.tensor(state_).to(self.q_eval.device)\n","\n","        q_pred = self.q_eval.forward(state_btch)[idxs, actions]\n","        q_next = self.q_next.forward(state_btch_)\n","        q_eval = self.q_eval.forward(state_btch_)\n","\n","        max_actions = T.argmax(q_eval, dim=1)\n","\n","        # # apply mask for terminates networks\n","        # q_next[{}] = 0.0\n","\n","        q_target = rewards_btch + self.gamma * q_next[idxs, max_actions]\n","\n","        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n","        loss.backward()\n","\n","        self.q_eval.optimiser.step()\n","        self.learn_step_counter += 1\n","        self.dec_epsilon()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"py6bwP7_wgZG","outputId":"50af3a92-65e3-436b-a401-6c7bfcef9324","executionInfo":{"status":"error","timestamp":1653515754931,"user_tz":-60,"elapsed":3509,"user":{"displayName":"HAN ZHANG","userId":"00984646561456990334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["...starting...\n","position: [(1, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -0.75\n","position: [(1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 3\n","score: -0.79\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2)]\n","action: 1\n","score: -1.09\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1)]\n","action: 1\n","score: -1.84\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1)]\n","action: 4\n","score: -1.8800000000000001\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1)]\n","action: 3\n","score: -2.63\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1)]\n","action: 2\n","score: -2.9299999999999997\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1)]\n","action: 1\n","score: -3.6799999999999997\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -4.43\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1)]\n","action: 0\n","score: -4.7299999999999995\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -5.4799999999999995\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -6.2299999999999995\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -6.9799999999999995\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 0\n","score: -7.279999999999999\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 2\n","score: -8.03\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 2\n","score: -8.78\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 3\n","score: -9.78\n","-----------------Next iteration-------------------\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -0.75\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -1.5\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -2.25\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 0\n","score: -2.55\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 2\n","score: -3.3\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 4\n","score: -3.5999999999999996\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1)]\n","action: 2\n","score: -3.8999999999999995\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1)]\n","action: 1\n","score: -4.6499999999999995\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -5.3999999999999995\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -6.1499999999999995\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 0\n","score: -6.449999999999999\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -7.199999999999999\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 2\n","score: -7.949999999999999\n","position: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (2, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n","action: 1\n","score: -8.7\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-4ecce0575416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_chck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-4ecce0575416>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(train_chck, chckpt, lr, epsilon, gamma, episodes, netname)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    613\u001b[0m                 )\n\u001b[1;32m    614\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     def send_multipart(\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","\n","def run(train_chck=True, chckpt=False, lr=0.01, epsilon=0.9,\n","        gamma=0.9, episodes=100, netname='default.pt'):\n","    # Default (Fixed) Parameters\n","    epsilon_min = 0.1\n","    epsilon_dec = 1e-6\n","    size = [9]\n","    input_dims = 9\n","    output_dims = 5\n","\n","    replace_testnet = 50\n","    memsize = 1000000 # https://arxiv.org/abs/1712.01275\n","    batch_size = 64\n","\n","    agent = Agent(gamma=gamma, epsilon=epsilon, lr=lr,\n","                  input_dims=input_dims,buffer_size= size, n_actions=output_dims, mem_size=memsize, eps_min=epsilon_min,\n","                  batch_size=batch_size, eps_dec=epsilon_dec, replace=replace_testnet)\n","\n","    if not train_chck:\n","        canv = Canvas()\n","    else:\n","        maze = load_maze()\n","\n","    # if chckpt:\n","    #     agent.load_models()\n","\n","    # plt = Plotter()\n","    env = Qmaze()\n","    print('...starting...')\n","    env.reset\n","    for i in range(episodes):\n","        \n","        done = False\n","        \n","        observation = env.observe_environment\n","\n","        if not train_chck:\n","            canv.set_visible(env.get_around_info.copy(), env.get_rat_pos, [])\n","\n","        score = 0\n","        while not done:\n","            canv.step(env.around_info.copy(), env.rat, env.rat_path)\n","            print(\"position:\",env.rat_path)\n","            action = agent.greedy_epsilon(observation)\n","            print(\"action:\",action)\n","            observation_, reward, done = env.step(action, score)\n","            score += reward\n","            print(\"score:\",score)\n","            agent.store_transition(observation, observation_, reward, action)\n","            agent.learn()\n","            observation = observation_\n","\n","            \n","            # print(done)\n","\n","        if env.rat==env.goal:\n","          print(\"sucessful\")\n","          break\n","        else:\n","          # env.animate_solution()\n","          print(\"-----------------Next iteration-------------------\")\n","    \n","    # env.animate_solution()\n"," \n","        # plt.data_in(score, wall_cntr=env.wall_cntr, stay_cntr=env.stay_cntr, visit_cntr=env.visit_cntr)\n","        # print(f'Ep {i}, score {score}, avg {plt.scores_avg[-1]}, epsilon {agent.epsilon}, lr {lr}')\n","        # print(f'    Stayed {env.stay_cntr} : Walls {env.wall_cntr}')\n","        # Save NN every 10 its\n","        # if i > 10 and i % 10 == 0:\n","        #     agent.save_models()\n","            # plt.live_plot()\n","\n","if __name__ == '__main__':\n","    run(train_chck=False, chckpt=True, episodes=1000, lr=0.0001, epsilon=0.5, gamma=0.90)\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"iGr2ZXjyjN7h"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"main_self.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"}},"nbformat":4,"nbformat_minor":0}